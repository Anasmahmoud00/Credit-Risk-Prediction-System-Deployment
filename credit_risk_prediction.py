# -*- coding: utf-8 -*-
"""credit-risk-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jluo8kl-sbtUFxJLrmwS0u32mS4iN5wJ

`This version doesn't depend on IterativeImputer_credit.csv`

# Import Liberaries
"""

import pandas as pd

pd.set_option('display.max_columns', None)

pd.set_option('display.max_rows', None)

import numpy as np

import matplotlib.pyplot as plt

import matplotlib.gridspec as gridspec

import seaborn as sns

import plotly.express as px

import plotly.graph_objects as go

from plotly.subplots import make_subplots

import plotly.tools as tls
import plotly.offline as py


import warnings

warnings.filterwarnings("ignore")

"""# Read Data"""

df = pd.read_csv('/kaggle/input/credit-risk-dataset/loan/loan.csv')

df.head(3)

"""# Data Preprocessing"""

null_percentages = df.isnull().mean() * 100



for column, percentage in null_percentages.items():

  if percentage:

    print({column: [round(percentage, 5), df[column].dtype]})

# Get the value counts for 'application_type'
application_type_counts = df['application_type'].value_counts()

# Set up the figure
plt.figure(figsize=(8, 5))

# Create the pie chart
plt.pie(application_type_counts.values, labels=application_type_counts.index, autopct='%1.1f%%',
        startangle=90, colors=sns.color_palette('Set2'))

# Add a legend
plt.legend(application_type_counts.index, title="Application Types", bbox_to_anchor=(1, 1), loc="best")

# Set title
plt.title('Distribution of Application Types')

# Display the plot
plt.show()

# Get the percentage of missing values for the 'JOINT' application type
joint_na_percentages = df[df['application_type'] == 'JOINT'].isna().mean() * 100

# Filter columns with more than 5% missing values
joint_na_percentages = joint_na_percentages[joint_na_percentages > 5]

# Set up the figure
plt.figure(figsize=(10, 5))

# Create the bar plot using Seaborn
sns.barplot(x=joint_na_percentages.values, y=joint_na_percentages.index, palette='Set2')

# Add labels and title
plt.xlabel('Percentage of Missing Values')
plt.ylabel('Column')
plt.title('Percentage of Missing Values for JOINT Application Type')

# Customize plot and background color
plt.gca().set_facecolor('lightgray')  # Background color inside the plot

# Show the plot
plt.show()

# Get the percentage of missing values for the 'INDIVIDUAL' application type
individual_na_percentages = df[df['application_type'] == 'INDIVIDUAL'].isna().mean() * 100

# Filter columns with more than 5% missing values
individual_na_percentages = individual_na_percentages[individual_na_percentages > 5]

# Set up the figure
plt.figure(figsize=(10, 6))

# Create the bar plot using Seaborn
sns.barplot(x=individual_na_percentages.values, y=individual_na_percentages.index, palette='Set2')

# Add labels and title
plt.xlabel('Percentage of Missing Values')
plt.ylabel('Column')
plt.title('Percentage of Missing Values for INDIVIDUAL Application Type')

# Customize plot and background color
plt.gca().set_facecolor('lightgray')  # Background color inside the plot

# Show the plot
plt.show()

df.shape

"""drop JOINT records to keep all values for INDIVIDUAL application_type"""

df = df[df['application_type'] != 'JOINT']

df.shape

columns_to_drop = [i for i in df.columns if 'joint' in i]

columns_to_drop.append('application_type')

df.drop(columns=columns_to_drop, inplace=True)

# Get the percentage of missing values for the entire DataFrame
na_percentages = df.isna().mean() * 100

# Filter columns with more than 5% missing values
na_percentages = na_percentages[na_percentages > 5]

# Set up the figure
plt.figure(figsize=(10, 6))

# Create the bar plot using Seaborn
sns.barplot(x=na_percentages.values, y=na_percentages.index, palette='Set2')

# Add labels and title
plt.xlabel('Percentage of Missing Values')
plt.ylabel('Column')
plt.title('Percentage of Missing Values in the Whole Data')

# Customize plot and background color
plt.gca().set_facecolor('lightgray')  # Background color inside the plot

# Show the plot
plt.show()

columns_to_drop = []

for i in df.columns:

  if df[i].isna().mean()*100 > 20:

    columns_to_drop.append(i)

df.drop(columns=columns_to_drop, inplace=True)

for i in df.columns:

  print({i: df[i].nunique()})

for i in df.columns:

    if df[i].nunique() < 10:

        print({i: df[i].value_counts()})

df.drop(columns=['pymnt_plan', 'policy_code', 'acc_now_delinq'], inplace=True)

df.head(3)

df[['grade', 'sub_grade']].head(5)

df['grade'].unique()

sub_grades = df['sub_grade'].unique()

sub_grades.sort()

sub_grades

df.drop(columns=['id', 'member_id', 'url', 'issue_d', 'earliest_cr_line', 'grade', 'last_credit_pull_d'], inplace=True)

df.shape

df.head(3)

df.drop(columns=['emp_title', 'title', 'zip_code', 'addr_state', 'last_pymnt_d'], inplace=True)

df.shape

# Set up the figure
plt.figure(figsize=(10, 6))

# Create the histogram using Seaborn
sns.histplot(data=df, x="emp_length", hue="loan_status", multiple="dodge", palette='Set2')

# Add title and axis labels
plt.title("Relationship between Employment Length and Loan Status")
plt.xlabel("Employment Length")
plt.ylabel("Count")

# Show the plot
plt.show()

df.drop(columns=['emp_length'], inplace=True)

importante_features = df.columns

len(importante_features)

terms = pd.read_excel('/kaggle/input/credit-risk-dataset/LCDataDictionary.xlsx')

terms.columns

feature_description = terms[terms['LoanStatNew'].isin(importante_features)][['LoanStatNew', 'Description']]

feature_description.reset_index(drop=True, inplace=True)

# Get the percentage of missing values for the entire DataFrame
na_percentages = df.isna().mean() * 100

# Filter columns with any missing values
na_percentages = na_percentages[na_percentages > 0]

# Set up the figure
plt.figure(figsize=(10, 6))

# Create the bar plot using Seaborn
sns.barplot(y=na_percentages.values, x=na_percentages.index, palette='Set2')

# Add labels and title
plt.xlabel('Percentage of Missing Values')
plt.ylabel('Features')
plt.title('Distribution of Nulls in Final Features')

# Rotate x-axis labels
plt.xticks(rotation=45, ha='right')  # Rotate labels and align them to the right

# Customize background color
plt.gca().set_facecolor('lightgray')  # Background color inside the plot
plt.gcf().set_facecolor('lightblue')  # Figure background color

# Display the plot
plt.show()

df[na_percentages.index].info()

na_percentages

print(df.shape)

for i in na_percentages.index:

  if na_percentages[i] < 1:

    df.dropna(subset=[i], inplace=True)

print(df.shape)

"""## EDA and Visualizations"""

importante_features

features = [col for col in df.columns if df[col].nunique() > 2]



rows = (len(features) + 2) // 3

cols = 3



# Set up the figure and axis for subplots

fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))

axes = axes.flatten()  # Flatten in case of multiple rows



fig.patch.set_facecolor('lightgray')  # Set the background color of the entire figure



palette = sns.color_palette('Set2', len(features))



plotted_df = df.copy()

plotted_df['diff_loan_funded'] = plotted_df['loan_amnt'] - plotted_df['funded_amnt']

for i, col in enumerate(features):

    sns.histplot(x=plotted_df[col], kde=False, ax=axes[i], color=palette[i], alpha=1)  # Set alpha slightly transparent for better visualization

    axes[i].set_title(col)



# Remove any empty subplots (in case the number of features doesn't fill the grid)

for j in range(i + 1, len(axes)):

    fig.delaxes(axes[j])



fig.suptitle("Distributions of Features", fontsize=16)

plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for the main title



plt.show()

plt.figure(figsize=(10, 6))

sns.boxplot(x='int_rate', y='loan_status', data=df)



plt.title('Loan Status by Interest Rate', fontsize=14)

plt.xlabel('Interest Rate (%)', fontsize=12)

plt.ylabel('Loan Status', fontsize=12)



plt.show()

plt.figure(figsize=(10, 6))



sns.boxplot(x='dti', y='loan_status', data=df, color='#8A3C9D')



plt.title('Loan Status by Debt-to-Income Ratio (DTI)')

plt.xlabel('Debt-to-Income Ratio (DTI)')

plt.ylabel('Loan Status')



plt.show()

avg_income = df.groupby('loan_status')['annual_inc'].mean().reset_index()



plt.figure(figsize=(10, 6))

sns.barplot(y='annual_inc',x='loan_status', data=avg_income)



plt.title('Loan Status by Average Annual Income', fontsize=14)

plt.xlabel('Loan Status', fontsize=12)

plt.ylabel('Average Annual Income', fontsize=12)

plt.xticks(rotation=45, ha='right')



plt.show()

loans_per_term = df['term'].value_counts().reset_index()



count_data = df.groupby(['term', 'loan_status']).size().reset_index(name='count')

count_data['percentage'] = count_data['count'] / count_data.groupby('term')['count'].transform('sum') * 100



fig = plt.figure(figsize=(16, 6))

gs = gridspec.GridSpec(1, 2, width_ratios=[1, 4])



ax1 = plt.subplot(gs[0])

sns.barplot(x='term', y='count', data=loans_per_term, ax=ax1, width=0.4)

ax1.set_title('Loans per Term', fontsize=14)

ax1.set_xlabel('Loan Term', fontsize=12)

ax1.set_ylabel('Number of Loans', fontsize=12)



ax2 = plt.subplot(gs[1])

sns.barplot(x='term', y='percentage', hue='loan_status', data=count_data, ax=ax2, width=0.8)

for p in ax2.patches:

    height = p.get_height()

    if height > 0:

        ax2.text(

            p.get_x() + p.get_width() / 2.,

            height,

            f'{height:.2f}',

            ha='center',

            va='bottom'

        )

ax2.set_title('Loan Status by Loan Term', fontsize=14)

ax2.set_xlabel('Loan Term', fontsize=12)

ax2.set_ylabel('Percentage per Loan Status (%)', fontsize=12)



ax2.legend( fontsize='small')



plt.show()

subgrade_counts = df['sub_grade'].value_counts().sort_index()



subgrade_proportions = df.groupby(['sub_grade', 'loan_status']).size().unstack(fill_value=0)

subgrade_proportions = subgrade_proportions.div(subgrade_proportions.sum(axis=1), axis=0)



fig, axes = plt.subplots(2, 1, figsize=(14, 12))

ax1, ax2 = axes[0], axes[1]



sns.barplot(x=subgrade_counts.index, y=subgrade_counts.values, ax=ax1, width=0.6)

ax1.set_title('Number of Loans per Subgrade', fontsize=14)

ax1.set_xlabel('Subgrade', fontsize=12)

ax1.set_ylabel('Number of Loans', fontsize=12)

ax1.tick_params(axis='x', rotation=45)



subgrade_proportions.plot(kind='bar', stacked=True, ax=ax2, color=sns.color_palette('Dark2'))

ax2.set_title('Loan Status by Subgrade', fontsize=14)

ax2.set_xlabel('Subgrade', fontsize=12)

ax2.set_ylabel('Proportion', fontsize=12)

ax2.tick_params(axis='x', rotation=45)

ax2.legend(title='Loan Status', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')



plt.tight_layout()

plt.show()

charges_off_df = df[df['loan_status'].str.contains('Charged Off')]



plt.figure(figsize=(8, 4))

sns.boxplot(x='recoveries', y='loan_status', data=charges_off_df)



plt.title('Recovery Amount of Charged-off Loans', fontsize=14)

plt.ylabel('Loan Status', fontsize=12)

plt.xlabel('Recovery Amount', fontsize=12)



plt.show()

purpose_status_counts = df.groupby(['purpose', 'loan_status']).size().unstack(fill_value=0)



plt.figure(figsize=(8, 10))

purpose_status_counts.plot(kind='bar', stacked=True, figsize=(12, 6), color=sns.color_palette('Dark2'))



plt.title('Loan Status by Purpose', fontsize=14)

plt.xlabel('Purpose of Loan', fontsize=12)

plt.ylabel('Number of Loans', fontsize=12)

plt.xticks(rotation=45)



plt.show()

"""## Encoding"""

categorical_features = df.select_dtypes(include='object').drop(columns=['loan_status'])

categorical_features.head()

for i in categorical_features.columns:

  print({i: categorical_features[i].value_counts()})

def sub_grades_encoding(x):

  val = 0

  if 'A' in x:

    val = 7

  elif 'B' in x:

    val = 6

  elif 'C' in x:

    val = 5

  elif 'D' in x:

    val = 4

  elif 'E' in x:

    val = 3

  elif 'F' in x:

    val = 2

  elif 'G' in x:

    val = 1



  if '1' in x:

    val += 0.8

  elif '2' in x:

    val += 0.6

  elif '3' in x:

    val += 0.4

  elif '4' in x:

    val += 0.2

  elif '5' in x:

    val += 0.0



  return val



def verification_status(x):

  if x == 'Not Verified':

    return 0

  return 1

df['sub_grade'] = df['sub_grade'].apply(sub_grades_encoding)

df['sub_grade'].unique()

from sklearn.preprocessing import LabelEncoder



label_encoder = LabelEncoder()



df['term'] = label_encoder.fit_transform(df['term'])

df['initial_list_status'] = label_encoder.fit_transform(df['initial_list_status'])

df['verification_status'] = df['verification_status'].apply(verification_status)

categorical_features = df.select_dtypes(include='object').drop(columns=['loan_status'])

categorical_features.isna().sum()

encoded_features = pd.get_dummies(categorical_features, dtype=int)

encoded_features.head(3)

df = pd.concat([df, encoded_features], axis=1)

df.drop(columns=categorical_features.columns, inplace=True)

df.head(3)

df['loan_status'] = label_encoder.fit_transform(df['loan_status'])

"""## Handling Missing Values"""

df.isna().sum()

# Importing necessary libraries

from sklearn.experimental import enable_iterative_imputer  # To enable IterativeImputer

from sklearn.impute import IterativeImputer



# Initialize the IterativeImputer with optimized settings

imputer = IterativeImputer(max_iter=5, random_state=0)



# Fit and transform the data

imputed_data = imputer.fit_transform(df)



# Convert back to a DataFrame if needed

imputed_df = pd.DataFrame(imputed_data, columns=df.columns)

"""`the IterativeImputer_credit.csv holds the imputed data`"""

# imputed_df = pd.read_csv('/kaggle/input/credit-risk-handled-data/IterativeImputer_credit.csv')

imputed_df.isna().sum()

imputed_df.shape

imputed_df['inq_last_6mths'].value_counts()

"""# Split Data & Scaling"""

from sklearn.model_selection import train_test_split



x = imputed_df.drop(columns=['loan_status'])

y = imputed_df['loan_status']

x_rest, X, y_rest, Y = train_test_split(x, y, test_size=0.1, random_state=42, stratify=y, shuffle=True)

X.shape, Y.shape

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y, shuffle=True)

from sklearn.preprocessing import MinMaxScaler



scaler = MinMaxScaler()

x_train = scaler.fit_transform(x_train)

x_test = scaler.transform(x_test)

x_train = pd.DataFrame(x_train, columns=x.columns)

x_test = pd.DataFrame(x_test, columns=x.columns)

"""# Feature Reduction"""

from sklearn.decomposition import PCA



pca = PCA(n_components=len(x_train.columns))



pca.fit(x_train)

cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)

# Create a DataFrame for cumulative explained variance ratio
cvr = pd.DataFrame({
    'Number of Principal Components': range(1, len(cumulative_variance_ratio) + 1),
    'Cumulative Explained Variance Ratio': cumulative_variance_ratio
})

# Set up the figure
plt.figure(figsize=(10, 6))

# Create the line plot using Seaborn
sns.lineplot(data=cvr,
             x='Number of Principal Components',
             y='Cumulative Explained Variance Ratio',
             marker='o')  # Adding markers for clarity

# Add title and labels
plt.title('Cumulative Explained Variance Ratio by Number of Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')

# Add grid lines for better readability
plt.grid(True)

# Show the figure
plt.show()

pca = PCA(n_components=25)

x_train_pca = pca.fit_transform(x_train)

x_test_pca = pca.transform(x_test)

"""# Modeling

## RandomForest
"""

from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report



rf_classifier = RandomForestClassifier(n_estimators=500,  max_depth=7, random_state=42)

rf_classifier.fit(x_train_pca, y_train)

y_pred_train = rf_classifier.predict(x_train_pca)

cm = confusion_matrix(y_train, y_pred_train)

# Create a heatmap using Seaborn

plt.figure(figsize=(12, 6))  # Set the figure size

sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues', cbar=True)



# Add titles and labels

plt.title('Confusion Matrix')

plt.xlabel('Predicted Labels')

plt.ylabel('True Labels')

plt.show()

print(classification_report(y_train, y_pred_train))

f1_score(y_train, y_pred_train, average='weighted')

y_pred_test = rf_classifier.predict(x_test_pca)

cm = confusion_matrix(y_test, y_pred_test)

# Create a heatmap using Seaborn

plt.figure(figsize=(12, 6))  # Set the figure size

sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues', cbar=True)



# Add titles and labels

plt.title('Confusion Matrix')

plt.xlabel('Predicted Labels')

plt.ylabel('True Labels')

plt.show()

print(classification_report(y_test, y_pred_test))

f1_score(y_test, y_pred_test, average='weighted')

"""## XGBoost"""

from xgboost import XGBClassifier



model = XGBClassifier(

    n_estimators=500,        # Number of boosting rounds

    learning_rate=0.1,       # Step size shrinkage to prevent overfitting

    max_depth=3,             # Maximum depth of a tree

    subsample=0.8,           # Subsample ratio of the training data

    colsample_bytree=0.8,    # Subsample ratio of features for each tree

    gamma=0,                 # Minimum loss reduction required to make a further partition

    reg_lambda=1,            # L2 regularization term

    objective='multi:softmax' # For multiclass classification (use 'binary:logistic' for binary)

)

model.fit(x_train_pca, y_train)

y_pred_train_xgb = model.predict(x_train_pca)

cm = confusion_matrix(y_train, y_pred_train_xgb)

# Create a heatmap using Seaborn

plt.figure(figsize=(12, 6))  # Set the figure size

sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues', cbar=True)



# Add titles and labels

plt.title('Confusion Matrix')

plt.xlabel('Predicted Labels')

plt.ylabel('True Labels')

plt.show()

print(classification_report(y_train, y_pred_train_xgb))

f1_score(y_train, y_pred_train_xgb, average='weighted')

y_pred_test_xgb = model.predict(x_test_pca)

cm = confusion_matrix(y_test, y_pred_test_xgb)

# Create a heatmap using Seaborn

plt.figure(figsize=(12, 6))  # Set the figure size

sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues', cbar=True)



# Add titles and labels

plt.title('Confusion Matrix')

plt.xlabel('Predicted Labels')

plt.ylabel('True Labels')

plt.show()

print(classification_report(y_test, y_pred_test_xgb))

f1_score(y_test, y_pred_test_xgb, average='weighted')

label_encoder.classes_

"""## XGBoost on all data"""

x = imputed_df.drop(columns=['loan_status'])

y = imputed_df['loan_status']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y, shuffle=True)

x_train = scaler.fit_transform(x_train)

x_test = scaler.transform(x_test)

x_train = pd.DataFrame(x_train, columns=x.columns)

x_test = pd.DataFrame(x_test, columns=x.columns)

pca = PCA(n_components=25)

x_train_pca = pca.fit_transform(x_train)

x_test_pca = pca.transform(x_test)

model.fit(x_train_pca, y_train)

y_pred_train_xgb = model.predict(x_train_pca)
cm = confusion_matrix(y_train, y_pred_train_xgb)
# Create a heatmap using Seaborn
plt.figure(figsize=(12, 6))  # Set the figure size
sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues', cbar=True)

# Add titles and labels
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

print(classification_report(y_train, y_pred_train_xgb))

Accuracy_Train = accuracy_score(y_train, y_pred_train_xgb)
F1_Score_Train = f1_score(y_train, y_pred_train_xgb, average='weighted')

print(f"Accuracy Train: {Accuracy_Train}")
print(f"F1 Score Trian: {F1_Score_Train}")

y_pred_test_xgb = model.predict(x_test_pca)
cm = confusion_matrix(y_test, y_pred_test_xgb)
# Create a heatmap using Seaborn
plt.figure(figsize=(12, 6))  # Set the figure size
sns.heatmap(cm, annot=True, fmt='0.2f', cmap='Blues', cbar=True)

# Add titles and labels
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

print(classification_report(y_test, y_pred_test_xgb))

Accuracy_Test = accuracy_score(y_test, y_pred_test_xgb)
F1_Score_Test = f1_score(y_test, y_pred_test_xgb, average='weighted')

print(f"Accuracy Test: {Accuracy_Test}")
print(f"F1 Score Test: {F1_Score_Test}")

# Create a pie chart with a single slice representing the accuracy
plt.figure(figsize=(6, 6))  # Adjust size as needed
plt.pie([round(Accuracy_Train*100, 2), 100 - round(Accuracy_Train*100, 2)],
        colors=['green', 'lightgray'],
        startangle=90,
        autopct='%1.1f%%',
        wedgeprops={'edgecolor': 'white'})

# Add a circle in the center to create the donut effect
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Add accuracy text in the center
plt.text(0, 0, f'{round(Accuracy_Train*100, 2):.2f}%', ha='center', va='center', fontsize=14, fontweight='bold')

# Add a title
plt.title('Model Train Accuracy')

# Show the plot
plt.show()

plt.figure(figsize=(6, 6))  # Adjust size as needed
plt.pie([round(F1_Score_Train*100, 2), 100 - round(F1_Score_Train*100, 2)],
        colors=['green', 'lightgray'],
        startangle=90,
        autopct='%1.1f%%',
        wedgeprops={'edgecolor': 'white'})

# Add a circle in the center to create the donut effect
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Add accuracy text in the center
plt.text(0, 0, f'{round(F1_Score_Train*100, 2):.2f}%', ha='center', va='center', fontsize=14, fontweight='bold')

# Add a title
plt.title('Model Train F1 Weighted Avg')

# Show the plot
plt.show()

# Create a pie chart with a single slice representing the accuracy
plt.figure(figsize=(6, 6))  # Adjust size as needed
plt.pie([round(Accuracy_Test*100, 2), 100 - round(Accuracy_Test*100, 2)],
        colors=['darkblue', 'lightgray'],
        startangle=90,
        autopct='%1.1f%%',
        wedgeprops={'edgecolor': 'white'})

# Add a circle in the center to create the donut effect
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Add accuracy text in the center
plt.text(0, 0, f'{round(Accuracy_Test*100, 2):.2f}%', ha='center', va='center', fontsize=14, fontweight='bold')

# Add a title
plt.title('Model Test Accuracy')

# Show the plot
plt.show()

plt.figure(figsize=(6, 6))  # Adjust size as needed
plt.pie([round(F1_Score_Test*100, 2), 100 - round(F1_Score_Test*100, 2)],
        colors=['darkblue', 'lightgray'],
        startangle=90,
        autopct='%1.1f%%',
        wedgeprops={'edgecolor': 'white'})

# Add a circle in the center to create the donut effect
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Add accuracy text in the center
plt.text(0, 0, f'{round(F1_Score_Test*100, 2):.2f}%', ha='center', va='center', fontsize=14, fontweight='bold')

# Add a title
plt.title('Model Test F1 Weighted Avg')

# Show the plot
plt.show()